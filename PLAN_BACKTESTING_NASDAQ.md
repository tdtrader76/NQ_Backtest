# üìä PLAN DE BACKTESTING - FUTUROS NASDAQ
**Sistema de Backtesting en 4 Fases**

---

## üìã √çNDICE

1. [Visi√≥n General](#visi√≥n-general)
2. [Datos y Estructura](#datos-y-estructura)
3. [FASE 1: Preparaci√≥n y Validaci√≥n de Datos](#fase-1-preparaci√≥n-y-validaci√≥n-de-datos)
4. [FASE 2: An√°lisis de Comportamiento de Niveles](#fase-2-an√°lisis-de-comportamiento-de-niveles)
5. [FASE 3: An√°lisis Intradiario Granular](#fase-3-an√°lisis-intradiario-granular)
6. [FASE 4: Por Definir](#fase-4-por-definir)
7. [Stack Tecnol√≥gico](#stack-tecnol√≥gico)
8. [Estructura de Carpetas](#estructura-de-carpetas)

---

## üéØ VISI√ìN GENERAL

### Objetivo
Desarrollar un sistema robusto de backtesting para validar estrategias de trading basadas en niveles calculados a partir de datos hist√≥ricos de futuros del NASDAQ.

### Alcance
- **Instrumento:** Futuros NASDAQ (NQ)
- **Periodo:** 5 a√±os hist√≥ricos
- **Timeframes:** Diario y 1 minuto
- **Metodolog√≠a:** An√°lisis basado en niveles calculados

### Criterios de √âxito
- ‚úÖ 100% de datos validados e √≠ntegros
- ‚úÖ C√°lculos verificados manualmente en Excel
- ‚úÖ Estad√≠sticas con nivel de confianza ‚â•95%
- ‚úÖ Sistema reproducible y documentado

---

## üìÅ DATOS Y ESTRUCTURA

### Datos de Entrada

#### Datos Diarios
```
Formato: CSV
Columnas: Date, Open, High, Low, Close, Volume
Periodo: 5 a√±os (~1,260 registros)
Ejemplo:
2020-01-02;9150,00;9200,50;9145,25;9195,75;125000
```

#### Datos de 1 Minuto
```
Formato: CSV
Columnas: Date, Time, Open, High, Low, Close, Volume
Periodo: 5 a√±os (~491,400 registros)
Ejemplo:
2020-01-02; 20:01:00; 9150,00; 9200,50; 9145,25; 9195,75; 125000
```

### Archivos de Salida

```
üì¶ Proyecto-Trading/
‚îú‚îÄ‚îÄ üìÇ C:\Users\oscar\Documents\Proyecto-Trading\Github\NQ_Backtest
‚îÇ     ‚îÇ   
‚îú     ‚îú‚îÄ‚îÄ üìÇ Originales/
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ NQ_Daily_2020-2024.csv
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_2020-2024.csv
‚îÇ     ‚îú‚îÄ‚îÄ üìÇ Procesados/
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ NQ_Daily_Limpio.csv
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_Limpio.csv
‚îÇ     ‚îî‚îÄ‚îÄ üìÇ Calculados/
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Niveles_Diarios.csv
‚îÇ     ‚îÇ   ‚îú‚îÄ‚îÄ Niveles_Diarios.xlsx
‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ Subniveles_Intradiarios.csv
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄüìÇ Resultados/
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ üìÇ Fase1/
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ üìÇ Fase2/
‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ üìÇ Fase3/ 
‚îÇ     ‚îú‚îÄ‚îÄ üìÇ Scripts/
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ fase1_limpieza.py
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ fase1_validacion.py
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ fase1_calculos.py
‚îÇ     ‚îÇ    ‚îú‚îÄ‚îÄ fase2_analisis_niveles.py
‚îÇ     ‚îÇ    ‚îî‚îÄ‚îÄ fase3_analisis_intradiario.py
‚îÇ     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ üìÇ Logs/
           ‚îú‚îÄ‚îÄ limpieza.log
           ‚îú‚îÄ‚îÄ validacion.log
           ‚îî‚îÄ‚îÄ errores.log
    
```

---

## üî∑ FASE 1: PREPARACI√ìN Y VALIDACI√ìN DE DATOS

### Objetivo
Garantizar la integridad, correcci√≥n y organizaci√≥n de todos los datos antes de realizar c√°lculos o an√°lisis.

---

### üìä TAREA 1.1: An√°lisis Exploratorio de Datos Diarios

#### Pasos

**1.1.1 Carga Inicial**
```python
import pandas as pd

# Cargar datos
df_daily = pd.read_csv('NQ_Daily_2020-2024.csv')

# Inspecci√≥n inicial
print(df_daily.info())
print(df_daily.describe())
print(df_daily.head(20))
```

**1.1.2 Validaci√≥n Estructural**

- ‚úÖ Verificar Precios Open, High, Low, Close, Volume tienen al menos 5 cifras y dos decimales, ejemplo: 20000,25 
     (Los decimales solo pueden ser: ,00 - ,25 - ,50 - ,75)
- ‚úÖ Verificar columnas esperadas
- ‚úÖ Verificar tipos de datos
- ‚úÖ Identificar valores nulos
- ‚úÖ Detectar duplicados

**Checklist de Validaci√≥n:**
```
‚ñ° Columnas: Date, Open, High, Low, Close, Volume
‚ñ° Date es tipo datetime
‚ñ° OHLCV son tipo float/int
‚ñ° No hay valores nulos
‚ñ° No hay duplicados de fecha
‚ñ° Fechas en orden cronol√≥gico
```

**1.1.3 Validaci√≥n de L√≥gica de Mercado**
```python
# Crear columna de validaci√≥n
df_daily['Valid'] = (
    (df_daily['High'] >= df_daily['Low']) &
    (df_daily['High'] >= df_daily['Open']) &
    (df_daily['High'] >= df_daily['Close']) &
    (df_daily['Low'] <= df_daily['Open']) &
    (df_daily['Low'] <= df_daily['Close']) &
    (df_daily['Volume'] > 0)
)

# Identificar registros inv√°lidos
invalid = df_daily[~df_daily['Valid']]
print(f"Registros inv√°lidos: {len(invalid)}")
```

**1.1.4 Detecci√≥n de Anomal√≠as**
```python
# Calcular returns diarios
df_daily['Returns'] = df_daily['Close'].pct_change()

# Detectar outliers (Z-score > 4)
from scipy import stats
df_daily['Z_Score'] = np.abs(stats.zscore(df_daily['Returns'].dropna()))
outliers = df_daily[df_daily['Z_Score'] > 4]

**Salidas:**
- `Datos/Procesados/Daily_Report.xlsx` (resumen estad√≠stico)
- `Datos/Procesados/Daily_Anomalies.csv` (anomal√≠as detectadas)
- `Logs/fase1_exploracion_daily.log`

---

### üìä TAREA 1.2: Organizaci√≥n en Excel por A√±os

#### Objetivo
Crear archivo Excel con hojas separadas por a√±o para validaci√≥n visual y manual.

#### Implementaci√≥n

**Script: `crear_excel_anual.py`**
```python
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows

def crear_excel_anual(df, output_path):
    """
    Crea Excel con hojas por a√±o y validaci√≥n visual
    """
    writer = pd.ExcelWriter(output_path, engine='openpyxl')

    # A√±adir columna de a√±o
    df['Year'] = pd.to_datetime(df['Date']).dt.year

    # Crear hoja por cada a√±o
    for year in sorted(df['Year'].unique()):
        df_year = df[df['Year'] == year].copy()

        # Agregar columnas de validaci√≥n
        df_year['H>=L'] = df_year['High'] >= df_year['Low']
        df_year['Valid_Range'] = (
            (df_year['Open'].between(df_year['Low'], df_year['High'])) &
            (df_year['Close'].between(df_year['Low'], df_year['High']))
        )

        # Escribir en Excel
        df_year.to_excel(writer, sheet_name=str(year), index=False)

        # Obtener worksheet para formateo
        ws = writer.sheets[str(year)]

        # Aplicar formato condicional (resaltar errores en rojo)
        for row in range(2, len(df_year) + 2):
            if not df_year.iloc[row-2]['Valid_Range']:
                for col in range(1, 10):
                    ws.cell(row, col).fill = PatternFill(
                        start_color='FFCCCC',
                        end_color='FFCCCC',
                        fill_type='solid'
                    )

    # Crear hoja resumen
    summary = df.groupby('Year').agg({
        'Close': ['count', 'mean', 'std', 'min', 'max'],
        'Volume': ['sum', 'mean']
    }).round(2)
    summary.to_excel(writer, sheet_name='RESUMEN')

    writer.close()
    print(f"‚úÖ Excel creado: {output_path}")

# Ejecutar
df = pd.read_csv('Datos/Procesados/NQ_Daily_Limpio.csv')
crear_excel_anual(df, 'Resultados/Fase1/Datos_Diarios_por_A√±o.xlsx')
```

**Estructura del Excel:**

```
üìä Datos_Diarios_por_A√±o.xlsx
‚îú‚îÄ‚îÄ üìÑ RESUMEN (estad√≠sticas generales)
‚îú‚îÄ‚îÄ üìÑ 2020 (252 registros)
‚îú‚îÄ‚îÄ üìÑ 2021 (252 registros)
‚îú‚îÄ‚îÄ üìÑ 2022 (251 registros)
‚îú‚îÄ‚îÄ üìÑ 2023 (251 registros)
‚îî‚îÄ‚îÄ üìÑ 2024 (252 registros)
```

**Columnas por a√±o:**
| Date | Open | High | Low | Close | Volume | H>=L | Valid_Range |
|------|------|------|-----|-------|--------|------|-------------|

**Validaci√≥n Manual:**
- [ ] Revisar registros resaltados en rojo
- [ ] Verificar primeros y √∫ltimos registros de cada a√±o
- [ ] Comprobar que no hay gaps inesperados
- [ ] Validar que totales de RESUMEN son coherentes

---

### üßÆ TAREA 1.3: C√°lculo de Niveles

#### Pendiente: Definici√≥n de F√≥rmulas

**Placeholder para f√≥rmulas a proporcionar:**

Los niveles se calcular√°n seg√∫n las f√≥rmulas espec√≠ficas que se proporcionen. El sistema est√° preparado para implementar cualquier tipo de c√°lculo:

**Ejemplos de niveles comunes (a confirmar):**

- Niveles personalizados
- Expected move

**Estructura Gen√©rica del Script:**

```python
def calcular_niveles(df):
    """
    Calcula niveles seg√∫n f√≥rmulas proporcionadas

    Args:
        df: DataFrame con OHLCV diario

    Returns:
        DataFrame con columnas adicionales de niveles
    """
    df = df.copy()

    # F√ìRMULAS A IMPLEMENTAR AQU√ç
    # Ejemplo de estructura:

    # Nivel Superior
    df['Nivel_R2'] = (df['High'] + df['Low']) / 2 + (df['High'] - df['Low'])
    df['Nivel_R1'] = (df['High'] + df['Low']) / 2 + (df['High'] - df['Low']) * 0.5

    # Nivel Medio
    df['Pivot'] = (df['High'] + df['Low'] + df['Close']) / 3

    # Nivel Inferior
    df['Nivel_S1'] = (df['High'] + df['Low']) / 2 - (df['High'] - df['Low']) * 0.5
    df['Nivel_S2'] = (df['High'] + df['Low']) / 2 - (df['High'] - df['Low'])

    return df

# Aplicar c√°lculos
df_con_niveles = calcular_niveles(df_daily)

# Exportar
df_con_niveles.to_csv('Datos/Calculados/Niveles_Diarios.csv', index=False)
```

**Validaci√≥n de C√°lculos:**

Para asegurar que los c√°lculos son correctos:

1. **Validaci√≥n en Excel:**
   - Crear hoja con f√≥rmulas manuales
   - Comparar resultados con Python
   - Verificar en muestra aleatoria de 20 d√≠as

2. **Test unitarios:**
```python
def test_niveles():
    # Caso de prueba conocido
    test_data = pd.DataFrame({
        'High': [100],
        'Low': [90],
        'Close': [95]
    })

    result = calcular_niveles(test_data)

    # Verificar resultados esperados
    assert result['Pivot'][0] == 95.0  # Ejemplo
    assert result['Nivel_R1'][0] == 100.0  # Ejemplo
```

**Salidas:**
- `Datos/Calculados/Niveles_Diarios.csv`
- `Datos/Calculados/Niveles_Diarios.xlsx` (con formato visual)
- `Logs/fase1_calculos.log`

---

### üìä TAREA 1.4: An√°lisis de Datos de 1 Minuto

#### Objetivo
Validar integridad y formato de datos intradiarios antes de usarlos en Fase 3.

**Consideraciones Especiales:**
- **Volumen:** ~491,400 registros (procesamiento por chunks)
- **Memoria:** Usar tipos de datos eficientes (float32 en lugar de float64)
- **Sesiones:** Identificar horarios de trading (9:30-16:00 ET)

#### Pasos

**1.4.1 Carga Eficiente**
```python
# Cargar por chunks para no saturar memoria
chunks = []
chunk_size = 50000

for chunk in pd.read_csv('NQ_1min_2020-2024.csv', chunksize=chunk_size):
    # Optimizar tipos
    chunk['Date'] = pd.to_datetime(chunk['Date'])
    chunk['Time'] = pd.to_datetime(chunk['Time'], format='%H:%M:%S').dt.time

    for col in ['Open', 'High', 'Low', 'Close']:
        chunk[col] = chunk[col].astype('float32')
    chunk['Volume'] = chunk[col].astype('int32')

    chunks.append(chunk)

df_1min = pd.concat(chunks, ignore_index=True)
```

**1.4.2 Validaci√≥n de Sesiones**
```python
from datetime import time

# Definir horario de sesi√≥n regular
session_start = time(9, 30)
session_end = time(16, 0)

# Identificar registros fuera de sesi√≥n
df_1min['In_Session'] = df_1min['Time'].between(session_start, session_end)

# Analizar cobertura
coverage = df_1min.groupby(['Date', 'In_Session']).size().unstack(fill_value=0)
print("Registros por sesi√≥n:")
print(coverage)

# Esperado por d√≠a de sesi√≥n regular: 390 minutos
expected_bars = 390
actual_bars = df_1min[df_1min['In_Session']].groupby('Date').size()
missing_bars = expected_bars - actual_bars
```

**1.4.3 Validaci√≥n de Continuidad Temporal**
```python
# Verificar que no haya saltos de tiempo mayores a 1 minuto
df_1min['DateTime'] = pd.to_datetime(
    df_1min['Date'].astype(str) + ' ' + df_1min['Time'].astype(str)
)
df_1min = df_1min.sort_values('DateTime')

df_1min['Time_Diff'] = df_1min['DateTime'].diff()

# Identificar gaps > 1 minuto (excluyendo fines de semana)
gaps = df_1min[df_1min['Time_Diff'] > pd.Timedelta(minutes=1)]
gaps_filtered = gaps[gaps['DateTime'].dt.dayofweek < 5]  # Lunes=0, Viernes=4
```

**1.4.4 Resumen Estad√≠stico**
```python
# Crear resumen diario de datos de 1 minuto
daily_summary = df_1min.groupby('Date').agg({
    'Open': 'first',
    'High': 'max',
    'Low': 'min',
    'Close': 'last',
    'Volume': 'sum',
    'DateTime': 'count'  # N√∫mero de barras
}).rename(columns={'DateTime': 'Num_Bars'})

# Comparar con datos diarios
comparison = pd.merge(
    df_daily[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']],
    daily_summary,
    on='Date',
    suffixes=('_Daily', '_1min')
)

# Calcular diferencias
comparison['High_Diff'] = abs(comparison['High_Daily'] - comparison['High_1min'])
comparison['Low_Diff'] = abs(comparison['Low_Daily'] - comparison['Low_1min'])

# D√≠as con diferencias significativas (>0.5%)
discrepancies = comparison[
    (comparison['High_Diff'] / comparison['High_Daily'] > 0.005) |
    (comparison['Low_Diff'] / comparison['Low_Daily'] > 0.005)
]
```

**Salidas:**
- `Resultados/Fase1/1min_Validation_Report.xlsx`
- `Resultados/Fase1/1min_Gaps.csv`
- `Resultados/Fase1/1min_vs_Daily_Comparison.csv`
- `Logs/fase1_validacion_1min.log`

---

### ‚úÖ CRITERIOS DE ACEPTACI√ìN - FASE 1

**Para pasar a Fase 2, se debe cumplir:**

- [x] 100% de datos diarios validados
- [x] Excel por a√±os creado y revisado manualmente
- [x] C√°lculos de niveles implementados y verificados
- [x] Resultados de c√°lculos exportados en CSV y Excel
- [x] Datos de 1 minuto validados
- [x] Discrepancias documentadas y explicadas
- [x] Todos los logs generados y revisados

**Entregables:**
```
‚úÖ Datos/Procesados/NQ_Daily_Limpio.csv
‚úÖ Datos/Procesados/NQ_1min_Limpio.csv
‚úÖ Datos/Calculados/Niveles_Diarios.csv
‚úÖ Datos/Calculados/Niveles_Diarios.xlsx
‚úÖ Resultados/Fase1/Datos_Diarios_por_A√±o.xlsx
‚úÖ Resultados/Fase1/Informe_Validacion.pdf
```

---

## üî∂ FASE 2: AN√ÅLISIS DE COMPORTAMIENTO DE NIVELES

### Objetivo
Analizar estad√≠sticamente c√≥mo el precio del d√≠a siguiente (D+1) se comporta respecto a los niveles calculados el d√≠a anterior (D).

### Concepto Clave
```
D√≠a D: Calculamos niveles basados en OHLC de ese d√≠a
D√≠a D+1: Observamos c√≥mo el precio reacciona a esos niveles
```

---

### üìä TAREA 2.1: Preparaci√≥n de Dataset de An√°lisis

#### Estructura de Datos

**Script: `preparar_dataset_analisis.py`**

```python
def preparar_dataset_analisis(df_niveles):
    """
    Crea dataset con niveles de d√≠a D y precios de d√≠a D+1
    """
    df = df_niveles.copy()

    # Shift para tener precios del d√≠a siguiente
    df['Next_Date'] = df['Date'].shift(-1)
    df['Next_Open'] = df['Open'].shift(-1)
    df['Next_High'] = df['High'].shift(-1)
    df['Next_Low'] = df['Low'].shift(-1)
    df['Next_Close'] = df['Close'].shift(-1)
    df['Next_Volume'] = df['Volume'].shift(-1)

    # Eliminar √∫ltimo registro (no tiene d√≠a siguiente)
    df = df[:-1].copy()

    # Verificar que Next_Date es efectivamente el d√≠a siguiente
    df['Days_Gap'] = (pd.to_datetime(df['Next_Date']) -
                      pd.to_datetime(df['Date'])).dt.days

    # Alertar si hay gaps > 3 d√≠as (fines de semana + festivo)
    large_gaps = df[df['Days_Gap'] > 3]
    if len(large_gaps) > 0:
        print(f"‚ö†Ô∏è  {len(large_gaps)} gaps grandes detectados")

    return df

# Ejecutar
df_analisis = preparar_dataset_analisis(df_con_niveles)
df_analisis.to_csv('Datos/Calculados/Dataset_Analisis_Fase2.csv', index=False)
```

**Estructura del Dataset:**
| Date | Open | High | Low | Close | Nivel_R2 | Nivel_R1 | Pivot | Nivel_S1 | Nivel_S2 | Next_Date | Next_Open | Next_High | Next_Low | Next_Close |
|------|------|------|-----|-------|----------|----------|-------|----------|----------|-----------|-----------|-----------|----------|------------|

---

### üìä TAREA 2.2: An√°lisis de Respeto a Niveles

#### Definici√≥n de Comportamientos

**Clasificaci√≥n:**
1. **RESPETA**: El rango [Next_Low, Next_High] est√° completamente dentro del rango de niveles
2. **BREAKOUT ALCISTA**: Next_High supera el nivel superior
3. **BREAKOUT BAJISTA**: Next_Low rompe el nivel inferior
4. **PENETRACI√ìN Y RETORNO**: Supera nivel pero cierra dentro del rango

#### Implementaci√≥n

```python
def analizar_respeto_niveles(df, nivel_superior='Nivel_R1', nivel_inferior='Nivel_S1'):
    """
    Analiza c√≥mo el precio del d√≠a siguiente respeta los niveles
    """
    df = df.copy()

    # 1. Clasificar comportamiento
    df['Breakout_Alcista'] = df['Next_High'] > df[nivel_superior]
    df['Breakout_Bajista'] = df['Next_Low'] < df[nivel_inferior]
    df['Respeta_Rango'] = (
        (df['Next_High'] <= df[nivel_superior]) &
        (df['Next_Low'] >= df[nivel_inferior])
    )

    # 2. Calcular excursi√≥n si hay breakout
    df['Excursion_Alcista'] = df['Next_High'] - df[nivel_superior]
    df['Excursion_Alcista'] = df['Excursion_Alcista'].where(df['Breakout_Alcista'], 0)

    df['Excursion_Bajista'] = df[nivel_inferior] - df['Next_Low']
    df['Excursion_Bajista'] = df['Excursion_Bajista'].where(df['Breakout_Bajista'], 0)

    # 3. Detectar retornos
    df['Retorno_Alcista'] = (
        df['Breakout_Alcista'] &
        (df['Next_Close'] <= df[nivel_superior])
    )

    df['Retorno_Bajista'] = (
        df['Breakout_Bajista'] &
        (df['Next_Close'] >= df[nivel_inferior])
    )

    # 4. Calcular penetraci√≥n como % del rango
    rango_niveles = df[nivel_superior] - df[nivel_inferior]
    df['Penetracion_Alcista_Pct'] = (df['Excursion_Alcista'] / rango_niveles * 100)
    df['Penetracion_Bajista_Pct'] = (df['Excursion_Bajista'] / rango_niveles * 100)

    return df
```

---

### üìä TAREA 2.3: Estad√≠sticas y M√©tricas

#### Estad√≠sticas Generales

```python
def calcular_estadisticas_respeto(df):
    """
    Genera estad√≠sticas completas de respeto a niveles
    """
    total_dias = len(df)

    stats = {
        'Total_Dias': total_dias,
        'Dias_Respeta': df['Respeta_Rango'].sum(),
        'Dias_Breakout_Alcista': df['Breakout_Alcista'].sum(),
        'Dias_Breakout_Bajista': df['Breakout_Bajista'].sum(),
        'Pct_Respeta': df['Respeta_Rango'].sum() / total_dias * 100,
        'Pct_Breakout_Alcista': df['Breakout_Alcista'].sum() / total_dias * 100,
        'Pct_Breakout_Bajista': df['Breakout_Bajista'].sum() / total_dias * 100,
        'Tasa_Retorno_Alcista': (
            df['Retorno_Alcista'].sum() / df['Breakout_Alcista'].sum() * 100
            if df['Breakout_Alcista'].sum() > 0 else 0
        ),
        'Tasa_Retorno_Bajista': (
            df['Retorno_Bajista'].sum() / df['Breakout_Bajista'].sum() * 100
            if df['Breakout_Bajista'].sum() > 0 else 0
        ),
    }

    return pd.Series(stats)
```

#### An√°lisis de Excursiones

```python
def analizar_excursiones(df):
    """
    An√°lisis detallado de excursiones fuera de niveles
    """
    breakouts_alcistas = df[df['Breakout_Alcista']]
    breakouts_bajistas = df[df['Breakout_Bajista']]

    stats_alcista = {
        'Excursion_Media': breakouts_alcistas['Excursion_Alcista'].mean(),
        'Excursion_Mediana': breakouts_alcistas['Excursion_Alcista'].median(),
        'Excursion_StdDev': breakouts_alcistas['Excursion_Alcista'].std(),
        'Excursion_Max': breakouts_alcistas['Excursion_Alcista'].max(),
        'Penetracion_Media_Pct': breakouts_alcistas['Penetracion_Alcista_Pct'].mean(),
    }

    stats_bajista = {
        'Excursion_Media': breakouts_bajistas['Excursion_Bajista'].mean(),
        'Excursion_Mediana': breakouts_bajistas['Excursion_Bajista'].median(),
        'Excursion_StdDev': breakouts_bajistas['Excursion_Bajista'].std(),
        'Excursion_Max': breakouts_bajistas['Excursion_Bajista'].max(),
        'Penetracion_Media_Pct': breakouts_bajistas['Penetracion_Bajista_Pct'].mean(),
    }

    return {
        'Alcista': pd.Series(stats_alcista),
        'Bajista': pd.Series(stats_bajista)
    }
```

---

### üìä TAREA 2.4: An√°lisis por Contexto

#### Segmentaci√≥n por Volatilidad

```python
def analizar_por_volatilidad(df):
    """
    Segmenta an√°lisis seg√∫n r√©gimen de volatilidad
    """
    # Calcular ATR 14 d√≠as
    df['TR'] = df[['High', 'Low', 'Close']].apply(
        lambda x: max(
            x['High'] - x['Low'],
            abs(x['High'] - df['Close'].shift(1)),
            abs(x['Low'] - df['Close'].shift(1))
        ),
        axis=1
    )
    df['ATR_14'] = df['TR'].rolling(14).mean()

    # Clasificar en terciles
    df['Volatilidad'] = pd.qcut(
        df['ATR_14'],
        q=3,
        labels=['Baja', 'Media', 'Alta']
    )

    # Analizar por r√©gimen
    resultados = []
    for vol in ['Baja', 'Media', 'Alta']:
        df_vol = df[df['Volatilidad'] == vol]
        stats = calcular_estadisticas_respeto(df_vol)
        stats['Volatilidad'] = vol
        resultados.append(stats)

    return pd.DataFrame(resultados)
```

---

### ‚úÖ CRITERIOS DE ACEPTACI√ìN - FASE 2

- [ ] Dataset de an√°lisis creado
- [ ] Estad√≠sticas de respeto calculadas
- [ ] An√°lisis de excursiones completado
- [ ] An√°lisis por contexto realizado
- [ ] Dashboard Excel generado
- [ ] Insights documentados

**Entregables:**
```
‚úÖ Datos/Calculados/Dataset_Analisis_Fase2.csv
‚úÖ Resultados/Fase2/Dashboard_Analisis_Niveles.xlsx
‚úÖ Resultados/Fase2/Distribucion_Excursiones.png
‚úÖ Logs/fase2_analisis.log
```

---

## üî∑ FASE 3: AN√ÅLISIS INTRADIARIO GRANULAR

### Objetivo
Calcular subniveles dentro de los rangos diarios y analizar la reacci√≥n del precio en datos de 1 minuto.

---

### üìä TAREA 3.1: C√°lculo de Subniveles

#### Divisi√≥n Equidistante

```python
def calcular_subniveles_equidistantes(df, num_subniveles=4):
    """
    Divide cada segmento entre niveles en subniveles equidistantes
    """
    df = df.copy()

    segmentos = [
        ('Nivel_R2', 'Nivel_R1', 'Sub_R2_R1'),
        ('Nivel_R1', 'Pivot', 'Sub_R1_P'),
        ('Pivot', 'Nivel_S1', 'Sub_P_S1'),
        ('Nivel_S1', 'Nivel_S2', 'Sub_S1_S2'),
    ]

    for nivel_superior, nivel_inferior, prefijo in segmentos:
        rango = df[nivel_superior] - df[nivel_inferior]
        paso = rango / (num_subniveles + 1)

        for i in range(1, num_subniveles + 1):
            df[f'{prefijo}_{i}'] = df[nivel_inferior] + (paso * i)

    return df
```

---

### üìä TAREA 3.2: Detecci√≥n de Reacciones

#### Definici√≥n de Reacci√≥n

Una reacci√≥n es significativa si:
1. **Toque**: Precio llega al subnivel (¬± tolerancia)
2. **Rechazo**: Precio rebota ‚â• X puntos
3. **Consolidaci√≥n**: Permanece cerca ‚â• Y minutos

```python
def detectar_reacciones(df_1min, niveles_dia, tolerancia_puntos=2,
                       rechazo_minimo=10, tiempo_consolidacion=5):
    """
    Detecta reacciones a subniveles en datos de 1 minuto
    """
    reacciones = []

    # Obtener subniveles
    cols_subniveles = [col for col in niveles_dia.index
                       if 'Sub_' in col or 'Fib_' in col]

    for subnivel_col in cols_subniveles:
        nivel_precio = niveles_dia[subnivel_col]

        # Buscar toques
        df_1min['Distancia'] = df_1min.apply(
            lambda x: min(
                abs(x['High'] - nivel_precio),
                abs(x['Low'] - nivel_precio)
            ),
            axis=1
        )

        toques = df_1min[df_1min['Distancia'] <= tolerancia_puntos]

        for idx, toque in toques.iterrows():
            # Analizar rechazo en siguientes barras
            # (implementaci√≥n completa en scripts)
            pass

    return pd.DataFrame(reacciones)
```

---

### üìä TAREA 3.3: An√°lisis Estad√≠stico

```python
def analizar_efectividad_subniveles(df_reacciones):
    """
    Eval√∫a qu√© subniveles generan m√°s reacciones
    """
    stats = df_reacciones.groupby('Subnivel').agg({
        'Reaccion_Significativa': ['sum', 'count', 'mean'],
        'Magnitud_Rechazo': ['mean', 'median', 'std'],
        'Tiempo_Consolidacion': ['mean', 'median']
    })

    return stats.sort_values(('Reaccion_Significativa', 'mean'), ascending=False)
```

---

### ‚úÖ CRITERIOS DE ACEPTACI√ìN - FASE 3

- [ ] Subniveles calculados
- [ ] Detecci√≥n de reacciones implementada
- [ ] Estad√≠sticas de efectividad completadas
- [ ] Zonas clave identificadas

**Entregables:**
```
‚úÖ Datos/Calculados/Subniveles_Intradiarios.csv
‚úÖ Resultados/Fase3/Reacciones_Subniveles.csv
‚úÖ Resultados/Fase3/Efectividad_Subniveles.xlsx
‚úÖ Resultados/Fase3/Heatmap_Reacciones.png
```

---

## üî∏ FASE 4: BACKTESTING DE ESTRATEGIA COMPLETA

### Objetivo
Implementar y evaluar estrategias de trading basadas en los insights de las fases anteriores.

---

### üìä COMPONENTES CLAVE

#### 1. Definici√≥n de Estrategia

**Ejemplo: Estrategia de Rebote en Niveles**

```python
class EstrategiaReboteNiveles:
    """
    Estrategia que opera rebotes en niveles de soporte/resistencia
    """

    def __init__(self, nivel_entrada, stop_loss_pts, take_profit_pts):
        self.nivel_entrada = nivel_entrada
        self.stop_loss = stop_loss_pts
        self.take_profit = take_profit_pts
        self.posicion = None

    def evaluar_entrada(self, precio_actual, nivel):
        """
        L√≥gica de entrada basada en aproximaci√≥n al nivel
        """
        if abs(precio_actual - nivel) <= 5:  # Dentro de 5 puntos
            return True
        return False

    def evaluar_salida(self, precio_actual, precio_entrada):
        """
        L√≥gica de salida por TP o SL
        """
        ganancia = precio_actual - precio_entrada

        if ganancia >= self.take_profit:
            return 'TAKE_PROFIT'
        elif ganancia <= -self.stop_loss:
            return 'STOP_LOSS'

        return None
```

#### 2. Motor de Backtesting

```python
class BacktestEngine:
    """
    Motor principal de backtesting
    """

    def __init__(self, df_1min, df_niveles, estrategia, capital_inicial=100000):
        self.df_1min = df_1min
        self.df_niveles = df_niveles
        self.estrategia = estrategia
        self.capital = capital_inicial
        self.trades = []

    def ejecutar_backtest(self):
        """
        Ejecuta backtest completo
        """
        for fecha in self.df_niveles['Date'].unique():
            # Obtener niveles del d√≠a
            niveles_dia = self.df_niveles[
                self.df_niveles['Date'] == fecha
            ].iloc[0]

            # Obtener datos intradiarios
            df_dia = self.df_1min[self.df_1min['Date'] == fecha]

            # Simular trading del d√≠a
            self._simular_dia(df_dia, niveles_dia)

        return self._generar_reporte()

    def _simular_dia(self, df_dia, niveles_dia):
        """
        Simula operativa de un d√≠a
        """
        posicion_abierta = False

        for idx, bar in df_dia.iterrows():
            # L√≥gica de entrada si no hay posici√≥n
            if not posicion_abierta:
                # Verificar se√±al de entrada
                pass

            # L√≥gica de salida si hay posici√≥n
            else:
                # Verificar condiciones de salida
                pass

    def _generar_reporte(self):
        """
        Genera m√©tricas de performance
        """
        df_trades = pd.DataFrame(self.trades)

        metricas = {
            'Total_Trades': len(df_trades),
            'Trades_Ganadores': len(df_trades[df_trades['PnL'] > 0]),
            'Trades_Perdedores': len(df_trades[df_trades['PnL'] < 0]),
            'Win_Rate': len(df_trades[df_trades['PnL'] > 0]) / len(df_trades) * 100,
            'PnL_Total': df_trades['PnL'].sum(),
            'PnL_Medio': df_trades['PnL'].mean(),
            'Mejor_Trade': df_trades['PnL'].max(),
            'Peor_Trade': df_trades['PnL'].min(),
            'Profit_Factor': (
                df_trades[df_trades['PnL'] > 0]['PnL'].sum() /
                abs(df_trades[df_trades['PnL'] < 0]['PnL'].sum())
            ),
            'Drawdown_Maximo': self._calcular_max_drawdown(df_trades),
            'Sharpe_Ratio': self._calcular_sharpe(df_trades),
        }

        return metricas
```

#### 3. M√©tricas de Performance

```python
def analizar_performance(df_trades):
    """
    An√°lisis completo de performance de la estrategia
    """

    # M√©tricas b√°sicas
    total_trades = len(df_trades)
    ganadores = df_trades[df_trades['PnL'] > 0]
    perdedores = df_trades[df_trades['PnL'] < 0]

    # Win Rate
    win_rate = len(ganadores) / total_trades * 100

    # Expectancy (expectativa matem√°tica)
    avg_win = ganadores['PnL'].mean()
    avg_loss = abs(perdedores['PnL'].mean())
    expectancy = (win_rate/100 * avg_win) - ((1-win_rate/100) * avg_loss)

    # Profit Factor
    gross_profit = ganadores['PnL'].sum()
    gross_loss = abs(perdedores['PnL'].sum())
    profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')

    # R-M√∫ltiplo promedio
    df_trades['R_Multiple'] = df_trades['PnL'] / df_trades['Risk']
    avg_r_multiple = df_trades['R_Multiple'].mean()

    # M√°ximo drawdown
    df_trades['Capital_Acum'] = df_trades['PnL'].cumsum()
    df_trades['Peak'] = df_trades['Capital_Acum'].cummax()
    df_trades['Drawdown'] = df_trades['Peak'] - df_trades['Capital_Acum']
    max_drawdown = df_trades['Drawdown'].max()

    # Sharpe Ratio (anualizado)
    returns = df_trades['PnL'] / 100000  # Asumiendo capital base
    sharpe = (returns.mean() / returns.std()) * np.sqrt(252)

    # Consecutive wins/losses
    df_trades['Win'] = df_trades['PnL'] > 0
    df_trades['Streak'] = df_trades['Win'].ne(df_trades['Win'].shift()).cumsum()
    streaks = df_trades.groupby('Streak')['Win'].agg(['first', 'count'])
    max_consecutive_wins = streaks[streaks['first']]['count'].max()
    max_consecutive_losses = streaks[~streaks['first']]['count'].max()

    metricas = {
        'Total_Trades': total_trades,
        'Win_Rate': win_rate,
        'Avg_Win': avg_win,
        'Avg_Loss': avg_loss,
        'Expectancy': expectancy,
        'Profit_Factor': profit_factor,
        'Avg_R_Multiple': avg_r_multiple,
        'Max_Drawdown': max_drawdown,
        'Sharpe_Ratio': sharpe,
        'Max_Consecutive_Wins': max_consecutive_wins,
        'Max_Consecutive_Losses': max_consecutive_losses,
    }

    return pd.Series(metricas)
```

#### 4. Optimizaci√≥n de Par√°metros

```python
from itertools import product

def optimizar_parametros(df_1min, df_niveles, param_grid):
    """
    Grid search para optimizar par√°metros de estrategia

    Args:
        param_grid: Dict con rangos de par√°metros
        {
            'stop_loss': [10, 15, 20, 25],
            'take_profit': [20, 30, 40, 50],
            'tolerancia_entrada': [2, 3, 5]
        }
    """
    resultados = []

    # Generar todas las combinaciones
    keys = param_grid.keys()
    values = param_grid.values()

    for combination in product(*values):
        params = dict(zip(keys, combination))

        # Ejecutar backtest con estos par√°metros
        estrategia = EstrategiaReboteNiveles(**params)
        engine = BacktestEngine(df_1min, df_niveles, estrategia)
        metricas = engine.ejecutar_backtest()

        # Guardar resultados
        resultado = {**params, **metricas}
        resultados.append(resultado)

    df_resultados = pd.DataFrame(resultados)

    # Ordenar por m√©trica objetivo (ej: Sharpe Ratio)
    df_resultados = df_resultados.sort_values('Sharpe_Ratio', ascending=False)

    return df_resultados
```

#### 5. Walk-Forward Analysis

```python
def walk_forward_analysis(df_1min, df_niveles, ventana_train=252, ventana_test=63):
    """
    An√°lisis walk-forward para evitar overfitting

    Args:
        ventana_train: D√≠as para entrenar (1 a√±o)
        ventana_test: D√≠as para testear (3 meses)
    """
    resultados_wf = []

    fechas_unicas = sorted(df_niveles['Date'].unique())

    inicio_test = ventana_train

    while inicio_test + ventana_test <= len(fechas_unicas):
        # Per√≠odo de entrenamiento
        fechas_train = fechas_unicas[inicio_test-ventana_train:inicio_test]

        # Per√≠odo de test
        fechas_test = fechas_unicas[inicio_test:inicio_test+ventana_test]

        # Optimizar en training set
        df_train = df_niveles[df_niveles['Date'].isin(fechas_train)]
        mejores_params = optimizar_parametros(
            df_1min[df_1min['Date'].isin(fechas_train)],
            df_train,
            param_grid={'stop_loss': [10, 15, 20], 'take_profit': [20, 30, 40]}
        ).iloc[0]

        # Evaluar en test set
        df_test = df_niveles[df_niveles['Date'].isin(fechas_test)]
        estrategia = EstrategiaReboteNiveles(
            stop_loss_pts=mejores_params['stop_loss'],
            take_profit_pts=mejores_params['take_profit']
        )
        engine = BacktestEngine(
            df_1min[df_1min['Date'].isin(fechas_test)],
            df_test,
            estrategia
        )
        metricas_test = engine.ejecutar_backtest()

        resultados_wf.append({
            'Periodo_Test': f"{fechas_test[0]} - {fechas_test[-1]}",
            **metricas_test
        })

        # Avanzar ventana
        inicio_test += ventana_test

    return pd.DataFrame(resultados_wf)
```

---

### üìä TAREA 4.1: Implementaci√≥n de Estrategia Base

**Pasos:**
1. Definir reglas de entrada y salida
2. Implementar gesti√≥n de riesgo
3. Codificar l√≥gica en motor de backtest
4. Ejecutar backtest completo
5. Analizar resultados

---

### üìä TAREA 4.2: Optimizaci√≥n

**Pasos:**
1. Definir espacio de par√°metros
2. Ejecutar grid search
3. Analizar superficie de optimizaci√≥n
4. Seleccionar par√°metros √≥ptimos
5. Validar con walk-forward

---

### üìä TAREA 4.3: An√°lisis de Robustez

```python
def analisis_robustez(df_resultados_optimizacion):
    """
    Analiza qu√© tan robustos son los par√°metros √≥ptimos
    """

    # Identificar "meseta" de buenos resultados
    top_10_pct = df_resultados_optimizacion.nlargest(
        int(len(df_resultados_optimizacion) * 0.1),
        'Sharpe_Ratio'
    )

    # Analizar variabilidad de par√°metros en top 10%
    param_variability = {
        'stop_loss': {
            'mean': top_10_pct['stop_loss'].mean(),
            'std': top_10_pct['stop_loss'].std(),
            'range': top_10_pct['stop_loss'].max() - top_10_pct['stop_loss'].min()
        },
        'take_profit': {
            'mean': top_10_pct['take_profit'].mean(),
            'std': top_10_pct['take_profit'].std(),
            'range': top_10_pct['take_profit'].max() - top_10_pct['take_profit'].min()
        }
    }

    # Baja variabilidad = Par√°metros robustos
    # Alta variabilidad = Posible overfitting

    return param_variability
```

---

### ‚úÖ CRITERIOS DE ACEPTACI√ìN - FASE 4

- [ ] Estrategia base implementada
- [ ] Backtest completo ejecutado
- [ ] M√©tricas de performance calculadas
- [ ] Optimizaci√≥n de par√°metros realizada
- [ ] Walk-forward analysis completado
- [ ] An√°lisis de robustez documentado
- [ ] Dashboard final creado

**Entregables:**
```
‚úÖ Scripts/estrategia_base.py
‚úÖ Resultados/Fase4/Backtest_Completo.xlsx
‚úÖ Resultados/Fase4/Optimizacion_Parametros.csv
‚úÖ Resultados/Fase4/Walk_Forward_Results.xlsx
‚úÖ Resultados/Fase4/Equity_Curve.png
‚úÖ Resultados/Fase4/Informe_Final.pdf
```

---

## üõ†Ô∏è STACK TECNOL√ìGICO

### Lenguajes y Frameworks

```yaml
Python: 3.9+
  Librer√≠as Core:
    - pandas: Manipulaci√≥n de datos
    - numpy: C√°lculos num√©ricos
    - scipy: Estad√≠stica avanzada

  Visualizaci√≥n:
    - matplotlib: Gr√°ficos
    - seaborn: Gr√°ficos estad√≠sticos
    - plotly: Gr√°ficos interactivos

  Machine Learning (opcional):
    - scikit-learn: Clustering, clasificaci√≥n
    - statsmodels: Tests estad√≠sticos

  Excel:
    - openpyxl: Lectura/escritura Excel
    - xlsxwriter: Formateo avanzado
```

### Estructura de Scripts

```
Scripts/
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py          # Carga de datos
‚îÇ   ‚îú‚îÄ‚îÄ data_validator.py       # Validaciones
‚îÇ   ‚îú‚îÄ‚îÄ nivel_calculator.py     # C√°lculo de niveles
‚îÇ   ‚îî‚îÄ‚îÄ export_utils.py         # Exportaci√≥n
‚îú‚îÄ‚îÄ fase1_pipeline.py           # Pipeline completo Fase 1
‚îú‚îÄ‚îÄ fase2_analisis.py           # An√°lisis Fase 2
‚îú‚îÄ‚îÄ fase3_subniveles.py         # Subniveles Fase 3
‚îú‚îÄ‚îÄ fase4_backtest.py           # Backtesting Fase 4
‚îî‚îÄ‚îÄ main.py                     # Orquestador principal
```

---

## üìù MEJORES PR√ÅCTICAS

### 1. Versionado de Datos

```python
from datetime import datetime

def guardar_con_version(df, nombre_base):
    """
    Guarda archivo con timestamp para trazabilidad
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{nombre_base}_{timestamp}.csv"
    df.to_csv(filename, index=False)

    # Guardar tambi√©n versi√≥n "latest"
    df.to_csv(f"{nombre_base}_latest.csv", index=False)

    return filename
```

### 2. Logging Completo

```python
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('Logs/backtesting.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Usar en c√≥digo
logger.info("Iniciando c√°lculo de niveles")
logger.warning(f"Detectados {n} outliers")
logger.error("Error en validaci√≥n de datos")
```

### 3. Tests Unitarios

```python
import unittest

class TestNivelCalculator(unittest.TestCase):

    def setUp(self):
        self.test_data = pd.DataFrame({
            'High': [100, 110, 105],
            'Low': [90, 95, 92],
            'Close': [95, 105, 100]
        })

    def test_pivot_calculation(self):
        result = calcular_niveles(self.test_data)
        expected_pivot = (100 + 90 + 95) / 3
        self.assertAlmostEqual(result['Pivot'][0], expected_pivot)
```

---

## üéØ ROADMAP DE EJECUCI√ìN

### Semana 1-2: FASE 1
- [ ] Configurar entorno Python
- [ ] Cargar y validar datos diarios
- [ ] Crear Excel por a√±os
- [ ] Implementar c√°lculos de niveles
- [ ] Validar datos de 1 minuto

### Semana 3-4: FASE 2
- [ ] Preparar dataset de an√°lisis
- [ ] Calcular estad√≠sticas de respeto
- [ ] Analizar excursiones
- [ ] An√°lisis por contexto
- [ ] Generar dashboard

### Semana 5-6: FASE 3
- [ ] Calcular subniveles
- [ ] Implementar detecci√≥n de reacciones
- [ ] An√°lisis estad√≠stico
- [ ] Identificar zonas clave

### Semana 7-8: FASE 4
- [ ] Implementar motor de backtest
- [ ] Definir estrategia base
- [ ] Ejecutar backtest
- [ ] Optimizar par√°metros
- [ ] Walk-forward analysis
- [ ] Informe final

---

## üìä M√âTRICAS DE √âXITO GLOBAL

### Calidad de Datos
- ‚úÖ 0 errores de validaci√≥n
- ‚úÖ 100% cobertura temporal
- ‚úÖ Discrepancias < 0.1%

### Insights Estad√≠sticos
- ‚úÖ Intervalo de confianza ‚â• 95%
- ‚úÖ N > 1000 observaciones
- ‚úÖ Resultados replicables

### Performance de Estrategia
- üéØ Sharpe Ratio > 1.5
- üéØ Win Rate > 50%
- üéØ Profit Factor > 1.5
- üéØ Max Drawdown < 20%

---

## üìö RECURSOS ADICIONALES

### Libros Recomendados
- "Advances in Financial Machine Learning" - Marcos L√≥pez de Prado
- "Evidence-Based Technical Analysis" - David Aronson
- "Quantitative Trading" - Ernest Chan

### Papers Acad√©micos
- "The Profitability of Technical Analysis: A Review" (Park & Irwin, 2007)
- "Support and Resistance Levels" (Osler, 2000)

---

**Documento creado:** 2025-01-25
**Versi√≥n:** 1.0
**Autor:** Sistema de Backtesting NASDAQ

