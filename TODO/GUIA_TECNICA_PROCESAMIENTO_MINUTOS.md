# Gu√≠a T√©cnica: Procesamiento de Datos de Minutos

## Sistema de Backtesting NQ - Documentaci√≥n T√©cnica

---

## üìë Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Formato de Datos](#formato-de-datos)
4. [Scripts Implementados](#scripts-implementados)
5. [Flujo de Procesamiento](#flujo-de-procesamiento)
6. [Validaciones](#validaciones)
7. [Manejo de Errores](#manejo-de-errores)
8. [Optimizaci√≥n y Performance](#optimizaci√≥n-y-performance)
9. [Troubleshooting](#troubleshooting)
10. [Ejemplos de Uso](#ejemplos-de-uso)

---

## Introducci√≥n

### Prop√≥sito

Este documento describe el sistema de procesamiento de datos intradiarios (minuto a minuto) para el proyecto de backtesting de futuros del NASDAQ (NQ).

### Alcance

- Consolidaci√≥n de 20+ archivos de contratos trimestrales
- Validaci√≥n de 1.6M+ registros de datos de minutos
- Divisi√≥n por a√±os para optimizar el manejo
- Preparaci√≥n para an√°lisis de Fase 3

### Tecnolog√≠as Utilizadas

- **Python 3.9+**
- **pandas**: Manipulaci√≥n de datos
- **pathlib**: Manejo de rutas
- **logging**: Sistema de logs
- **datetime**: Manejo de timestamps

---

## Arquitectura del Sistema

### Estructura de Carpetas

```
NQ_Backtest/
‚îú‚îÄ‚îÄ datos brutos/
‚îÇ   ‚îî‚îÄ‚îÄ datos ninjatrader/
‚îÇ       ‚îî‚îÄ‚îÄ Minutos/          # Archivos .txt originales
‚îÇ           ‚îú‚îÄ‚îÄ NQ 03-21.Last.txt
‚îÇ           ‚îú‚îÄ‚îÄ NQ 06-21.Last.txt
‚îÇ           ‚îî‚îÄ‚îÄ ... (20 archivos)
‚îú‚îÄ‚îÄ Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ consolidar_datos_minutos.py
‚îÇ   ‚îî‚îÄ‚îÄ dividir_datos_minutos_por_anio.py
‚îú‚îÄ‚îÄ Procesados/
‚îÇ   ‚îú‚îÄ‚îÄ 2020/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_2020.csv
‚îÇ   ‚îú‚îÄ‚îÄ 2021/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_2021.csv
‚îÇ   ‚îî‚îÄ‚îÄ ... (hasta 2025)
‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_2020-2025_Limpio.csv
‚îú‚îÄ‚îÄ Originales/
‚îÇ   ‚îî‚îÄ‚îÄ NQ_1min_2020-2025.csv
‚îî‚îÄ‚îÄ Logs/
    ‚îú‚îÄ‚îÄ consolidacion_minutos.log
    ‚îî‚îÄ‚îÄ dividir_datos_minutos_por_anio.log
```

### Flujo de Datos

```
[Archivos .txt]
     ‚Üì
[consolidar_datos_minutos.py]
     ‚Üì
[Validaci√≥n + Limpieza]
     ‚Üì
[NQ_1min_2020-2025_Limpio.csv]
     ‚Üì
[dividir_datos_minutos_por_anio.py]
     ‚Üì
[Archivos por a√±o 2020-2025]
```

---

## Formato de Datos

### Formato de Entrada (NinjaTrader .txt)

**Sin headers, separador: `;`**

```
YYYYMMDD HHMMSS;Open;High;Low;Close;Volume
20201215 230100;12604;12608.5;12601.75;12607;214
20201215 230200;12607.5;12609.5;12602.5;12605.75;166
```

**Caracter√≠sticas:**
- Fecha y hora en un solo campo
- Formato: `YYYYMMDD HHMMSS` (sin separadores)
- Precios: decimales con punto
- Volumen: n√∫mero entero
- Sin quotes en campos de texto

### Formato de Salida (CSV)

**Con headers, separador: `,`**

```csv
DateTime,Open,High,Low,Close,Volume
2020-12-15 23:01:00,12604.0,12608.5,12601.75,12607.0,214
2020-12-15 23:02:00,12607.5,12609.5,12602.5,12605.75,166
```

**Caracter√≠sticas:**
- DateTime en formato ISO: `YYYY-MM-DD HH:MM:SS`
- Precios: float con precisi√≥n de 2 decimales
- Volumen: entero
- Headers incluidos

---

## Scripts Implementados

### 1. consolidar_datos_minutos.py

#### Prop√≥sito
Consolidar todos los archivos .txt de contratos trimestrales en un √∫nico CSV validado.

#### Funciones Principales

##### `cargar_archivo_minutos(filepath)`

**Entrada:** Path al archivo .txt
**Salida:** DataFrame con datos del archivo

```python
def cargar_archivo_minutos(filepath):
    """
    Carga un archivo de datos de minutos de NinjaTrader

    Args:
        filepath (Path): Ruta al archivo .txt

    Returns:
        DataFrame: Datos cargados con columnas [DateTime, Open, High, Low, Close, Volume]
        None: Si hay error en la carga
    """
```

**Proceso:**
1. Lee CSV con `pandas.read_csv(sep=';', header=None)`
2. Convierte timestamp con `pd.to_datetime(format='%Y%m%d %H%M%S')`
3. Maneja errores con `errors='coerce'`
4. Elimina timestamps inv√°lidos (NaT)
5. Reordena columnas
6. Registra estad√≠sticas en log

##### `consolidar_datos_minutos()`

**Entrada:** Ninguna (lee de carpeta configurada)
**Salida:** DataFrame consolidado

```python
def consolidar_datos_minutos():
    """
    Consolida todos los archivos de minutos en un solo DataFrame

    Returns:
        DataFrame: Datos consolidados y ordenados cronol√≥gicamente
        None: Si no se cargaron datos
    """
```

**Proceso:**
1. Busca archivos .txt con `glob("*.txt")`
2. Carga cada archivo con `cargar_archivo_minutos()`
3. Concatena con `pd.concat(ignore_index=True)`
4. Ordena por DateTime con `sort_values('DateTime')`
5. Elimina duplicados con `drop_duplicates(subset=['DateTime'], keep='first')`
6. Registra estad√≠sticas

**Estrategia de Duplicados:**
- `keep='first'`: Prioriza registros de contratos m√°s antiguos
- Justificaci√≥n: Mayor liquidez en contrato previo al rollover

##### `validar_datos(df)`

**Entrada:** DataFrame con datos
**Salida:** DataFrame con columna 'Valid' a√±adida

```python
def validar_datos(df):
    """
    Valida que los datos cumplan con las reglas de mercado

    Args:
        df (DataFrame): Datos a validar

    Returns:
        DataFrame: Mismo DataFrame con columna 'Valid' (bool)
    """
```

**Validaciones Aplicadas:**

| Validaci√≥n | Expresi√≥n | Descripci√≥n |
|------------|-----------|-------------|
| High >= Low | `df['High'] >= df['Low']` | M√°ximo debe ser mayor o igual que m√≠nimo |
| High >= Open | `df['High'] >= df['Open']` | M√°ximo debe incluir apertura |
| High >= Close | `df['High'] >= df['Close']` | M√°ximo debe incluir cierre |
| Low <= Open | `df['Low'] <= df['Open']` | M√≠nimo debe incluir apertura |
| Low <= Close | `df['Low'] <= df['Close']` | M√≠nimo debe incluir cierre |
| Volume > 0 | `df['Volume'] > 0` | Volumen positivo |
| Precios > 0 | `df['Open/High/Low/Close'] > 0` | Todos los precios positivos |
| Timestamps cronol√≥gicos | `df['DateTime'].diff() >= 0` | No retrocesos en el tiempo |

##### `detectar_gaps(df, reportar_log=True)`

**Entrada:** DataFrame ordenado, flag de reporte
**Salida:** DataFrame con gaps detectados (o None)

```python
def detectar_gaps(df, reportar_log=True):
    """
    Detecta gaps temporales significativos en los datos

    Args:
        df (DataFrame): Datos ordenados por DateTime
        reportar_log (bool): Si True, escribe gaps en el log

    Returns:
        DataFrame: Gaps detectados con clasificaci√≥n
        None: Si no hay gaps
    """
```

**Clasificaci√≥n de Gaps:**

| Tipo | Duraci√≥n | Descripci√≥n | Ejemplo |
|------|----------|-------------|---------|
| Normal | ‚â§ 5 minutos | No se reporta | Actividad regular |
| Cierre diario | 5-120 minutos | Normal | 17:00-18:00 ET |
| Fin de semana | 120-4320 min (72h) | Normal | Viernes-Lunes |
| Gap largo | > 4320 minutos | Requiere investigaci√≥n | Festivos, eventos |

##### `exportar_datos(df, filename)`

**Entrada:** DataFrame validado, nombre base
**Salida:** Archivos CSV generados

```python
def exportar_datos(df, filename):
    """
    Exporta datos a archivos CSV (original y limpio)

    Args:
        df (DataFrame): Datos a exportar
        filename (str): Nombre base del archivo (sin extensi√≥n)

    Returns:
        DataFrame: Datos limpios (solo v√°lidos)
    """
```

**Archivos Generados:**
1. **Original**: `Originales/{filename}.csv`
   - Incluye columna 'Valid'
   - Para auditor√≠a

2. **Limpio**: `Procesados/{filename}_Limpio.csv`
   - Solo registros v√°lidos
   - Sin columna 'Valid'
   - Para an√°lisis

**Estad√≠sticas Reportadas:**
- Total de registros
- Registros v√°lidos/inv√°lidos
- Rango temporal (inicio/fin)
- Duraci√≥n en d√≠as
- Cobertura temporal (%)
- Rangos de precios (min/max/promedio)
- Volumen total

##### `main()`

**Orquestador Principal**

```python
def main():
    """
    Funci√≥n principal - Orquesta el flujo completo

    Pasos:
        1. Consolidar archivos
        2. Validar datos
        3. Detectar gaps
        4. Exportar datos
    """
```

**Flujo de Ejecuci√≥n:**
```
[PASO 1/4: Consolidar archivos]
     ‚Üì
[PASO 2/4: Validar datos]
     ‚Üì
[PASO 3/4: Detectar gaps]
     ‚Üì
[PASO 4/4: Exportar datos]
     ‚Üì
[‚úÖ CONSOLIDACI√ìN COMPLETADA]
```

---

### 2. dividir_datos_minutos_por_anio.py

#### Prop√≥sito
Dividir el archivo consolidado en archivos m√°s manejables organizados por a√±o.

#### Funciones Principales

##### `cargar_datos_consolidados()`

```python
def cargar_datos_consolidados():
    """
    Carga el archivo consolidado de datos de minutos

    Returns:
        DataFrame: Datos consolidados con DateTime parseado
        None: Si hay error
    """
```

**Proceso:**
1. Lee CSV con `parse_dates=['DateTime']`
2. Valida carga exitosa
3. Muestra rango temporal
4. Retorna DataFrame

##### `dividir_por_anios(df)`

```python
def dividir_por_anios(df):
    """
    Divide el DataFrame por a√±os y guarda cada a√±o separado

    Args:
        df (DataFrame): Datos consolidados

    Side effects:
        - Crea carpetas por a√±o
        - Genera archivos CSV individuales
        - Registra estad√≠sticas en log
    """
```

**Proceso:**
1. Extrae a√±o: `df['Year'] = df['DateTime'].dt.year`
2. Obtiene a√±os √∫nicos
3. Para cada a√±o:
   - Filtra datos del a√±o
   - Elimina columna 'Year'
   - Crea carpeta si no existe
   - Exporta a CSV
   - Calcula estad√≠sticas
   - Registra en log

##### `generar_resumen(df)`

```python
def generar_resumen(df):
    """
    Genera un resumen estad√≠stico de la divisi√≥n por a√±os

    Args:
        df (DataFrame): Datos consolidados

    Side effects:
        - Registra tabla de estad√≠sticas en log
        - Muestra distribuci√≥n porcentual
    """
```

**Estad√≠sticas Generadas:**
- Count, min, max por a√±o
- Close: mean, min, max
- Volume: sum
- Porcentaje de registros por a√±o

---

## Flujo de Procesamiento

### Flujo Completo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. CARGA DE ARCHIVOS               ‚îÇ
‚îÇ  - Glob 20 archivos .txt            ‚îÇ
‚îÇ  - Conversi√≥n timestamp             ‚îÇ
‚îÇ  - Validaci√≥n inicial               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. CONSOLIDACI√ìN                   ‚îÇ
‚îÇ  - Concatenar DataFrames            ‚îÇ
‚îÇ  - Ordenar cronol√≥gicamente         ‚îÇ
‚îÇ  - Eliminar duplicados              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. VALIDACI√ìN                      ‚îÇ
‚îÇ  - Reglas de mercado (OHLCV)        ‚îÇ
‚îÇ  - Timestamps cronol√≥gicos          ‚îÇ
‚îÇ  - Marcar registros inv√°lidos       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. DETECCI√ìN DE GAPS               ‚îÇ
‚îÇ  - Calcular diferencias temporales  ‚îÇ
‚îÇ  - Clasificar gaps                  ‚îÇ
‚îÇ  - Reportar top 10                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. EXPORTACI√ìN                     ‚îÇ
‚îÇ  - Archivo original (con Valid)     ‚îÇ
‚îÇ  - Archivo limpio (solo v√°lidos)    ‚îÇ
‚îÇ  - Estad√≠sticas completas           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. DIVISI√ìN POR A√ëOS               ‚îÇ
‚îÇ  - Extraer a√±o del DateTime         ‚îÇ
‚îÇ  - Crear carpetas                   ‚îÇ
‚îÇ  - Exportar archivos individuales   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Validaciones

### Validaciones de Datos de Mercado

#### Precios OHLC

```python
# Validaci√≥n High-Low
valid_hl = df['High'] >= df['Low']
# Contexto: El m√°ximo debe ser >= m√≠nimo por definici√≥n

# Validaci√≥n High-Open-Close
valid_hoc = (
    (df['High'] >= df['Open']) &
    (df['High'] >= df['Close'])
)
# Contexto: El m√°ximo debe contener apertura y cierre

# Validaci√≥n Low-Open-Close
valid_loc = (
    (df['Low'] <= df['Open']) &
    (df['Low'] <= df['Close'])
)
# Contexto: El m√≠nimo debe contener apertura y cierre
```

#### Volumen

```python
valid_volume = df['Volume'] > 0
# Contexto: Volumen debe ser positivo para trading real
```

#### Precios Positivos

```python
valid_prices = (
    (df['Open'] > 0) &
    (df['High'] > 0) &
    (df['Low'] > 0) &
    (df['Close'] > 0)
)
# Contexto: Precios negativos no tienen sentido en mercados de futuros
```

#### Timestamps Cronol√≥gicos

```python
df['TimeDiff'] = df['DateTime'].diff()
backward_jumps = (df['TimeDiff'] < timedelta(0)).sum()
if backward_jumps > 0:
    df.loc[df['TimeDiff'] < timedelta(0), 'Valid'] = False
# Contexto: El tiempo no puede retroceder
```

---

## Manejo de Errores

### Errores Durante la Carga

```python
try:
    df = pd.read_csv(filepath, sep=';', header=None, names=columns)
except Exception as e:
    logger.error(f"‚ùå Error al cargar {filepath}: {str(e)}")
    return None
```

**Tipos de Errores Manejados:**
- Archivo no encontrado
- Formato incorrecto
- Permisos insuficientes
- Memoria insuficiente

### Timestamps Inv√°lidos

```python
df['DateTime'] = pd.to_datetime(
    df['DateTimeStr'],
    format='%Y%m%d %H%M%S',
    errors='coerce'  # Convierte inv√°lidos a NaT
)

invalid_timestamps = df['DateTime'].isna().sum()
if invalid_timestamps > 0:
    logger.warning(f"‚ö†Ô∏è  {invalid_timestamps} timestamps inv√°lidos")
    df = df.dropna(subset=['DateTime'])
```

### Datos Vac√≠os

```python
if len(dfs) == 0:
    logger.error("‚ùå No se cargaron datos")
    return None
```

---

## Optimizaci√≥n y Performance

### Uso de Memoria

**Carga Completa vs Chunks:**

```python
# Actual: Carga completa
df = pd.read_csv(archivo)  # ~200-300 MB para 1.6M registros

# Alternativa si memoria es limitada:
chunks = []
for chunk in pd.read_csv(archivo, chunksize=100000):
    # Procesar chunk
    chunks.append(chunk)
df = pd.concat(chunks)
```

**Ventajas Carga Completa:**
- M√°s r√°pido
- C√≥digo m√°s simple
- Suficiente para este volumen

**Cu√°ndo Usar Chunks:**
- Datos > 5M registros
- Memoria RAM < 8GB
- Procesamiento en servidor limitado

### Operaciones Vectorizadas

```python
# Eficiente: Operaciones vectorizadas
df['Valid'] = (
    (df['High'] >= df['Low']) &  # pandas vectorizado
    (df['Volume'] > 0)
)

# Ineficiente: Loops
for idx, row in df.iterrows():  # EVITAR
    if row['High'] >= row['Low']:
        df.at[idx, 'Valid'] = True
```

### Ordenamiento y Duplicados

```python
# Optimizado
df = df.sort_values('DateTime').reset_index(drop=True)
df = df.drop_duplicates(subset=['DateTime'], keep='first')

# Alternativa con hash (si muchos duplicados)
df = df.drop_duplicates(subset=['DateTime'], keep='first', ignore_index=True)
```

---

## Troubleshooting

### Problemas Comunes

#### 1. Timestamps Inv√°lidos

**S√≠ntoma:**
```
‚ö†Ô∏è  NQ 03-21.Last.txt: 150 timestamps inv√°lidos
```

**Causa:**
- Formato incorrecto en archivo origen
- Caracteres no num√©ricos en fecha/hora
- Fechas imposibles (ej: 2021-02-30)

**Soluci√≥n:**
```python
# El script maneja autom√°ticamente con errors='coerce'
df['DateTime'] = pd.to_datetime(df['DateTimeStr'], errors='coerce')
df = df.dropna(subset=['DateTime'])
```

#### 2. Duplicados Excesivos

**S√≠ntoma:**
```
‚ö†Ô∏è  50,000 timestamps duplicados eliminados
```

**Causa:**
- Overlap largo entre contratos
- Datos repetidos en archivos origen

**Soluci√≥n:**
- Verificar archivos origen
- Confirmar estrategia `keep='first'` es apropiada
- Revisar logs para identificar archivos problem√°ticos

#### 3. Gaps Largos Inesperados

**S√≠ntoma:**
```
‚ö†Ô∏è  Gap largo: 2021-05-15 - Gap: 150000 min
```

**Causa:**
- Datos faltantes en archivo origen
- Problema en recolecci√≥n de datos
- Evento de mercado (suspensi√≥n)

**Soluci√≥n:**
1. Verificar archivo origen correspondiente
2. Consultar calendario de mercado para el periodo
3. Si es error de datos, corregir archivo origen

#### 4. Memoria Insuficiente

**S√≠ntoma:**
```
MemoryError: Unable to allocate array
```

**Causa:**
- RAM insuficiente
- Otros procesos consumiendo memoria

**Soluci√≥n:**
```python
# Modificar script para usar chunks
chunks = []
for chunk in pd.read_csv(archivo, chunksize=50000):
    # Procesar chunk
    chunks.append(chunk)
df = pd.concat(chunks, ignore_index=True)
```

#### 5. Archivo CSV Corrupto

**S√≠ntoma:**
```
‚ùå Error al cargar archivo: CParserError
```

**Causa:**
- Archivo da√±ado
- Formato inconsistente
- Caracteres especiales

**Soluci√≥n:**
1. Abrir archivo en editor de texto
2. Verificar formato visual
3. Buscar l√≠neas problem√°ticas
4. Corregir manualmente o contactar proveedor de datos

---

## Ejemplos de Uso

### Ejemplo 1: Ejecuci√≥n B√°sica

```bash
cd "C:\Users\oscar\Documents\Proyecto-Trading\Github\NQ_Backtest\Scripts"
python consolidar_datos_minutos.py
```

**Output Esperado:**
```
2025-12-04 10:02:11 - INFO - PASO 1/4: Consolidando archivos...
2025-12-04 10:02:11 - INFO - Archivos encontrados: 20
2025-12-04 10:02:11 - INFO - Archivo cargado: NQ 03-21.Last.txt (88776 registros)
...
2025-12-04 10:02:36 - INFO - ‚úÖ CONSOLIDACI√ìN COMPLETADA EXITOSAMENTE
```

### Ejemplo 2: Divisi√≥n por A√±os

```bash
cd "C:\Users\oscar\Documents\Proyecto-Trading\Github\NQ_Backtest\Scripts"
python dividir_datos_minutos_por_anio.py
```

**Output Esperado:**
```
2025-12-04 10:19:23 - INFO - Archivo cargado exitosamente: 1,612,055 registros
2025-12-04 10:19:25 - INFO - A√±os encontrados: [2020, 2021, 2022, 2023, 2024, 2025]
...
2025-12-04 10:19:29 - INFO - ‚úÖ DIVISI√ìN COMPLETADA EXITOSAMENTE
```

### Ejemplo 3: Verificaci√≥n de Datos

```python
import pandas as pd

# Cargar datos consolidados
df = pd.read_csv('../Procesados/NQ_1min_2020-2025_Limpio.csv',
                 parse_dates=['DateTime'])

# Verificar rango temporal
print(f"Fecha inicio: {df['DateTime'].min()}")
print(f"Fecha fin: {df['DateTime'].max()}")
print(f"Total registros: {len(df):,}")

# Verificar datos por a√±o
print("\nRegistros por a√±o:")
print(df['DateTime'].dt.year.value_counts().sort_index())

# Verificar continuidad
df['Gap'] = df['DateTime'].diff()
gaps_grandes = df[df['Gap'] > pd.Timedelta(hours=24)]
print(f"\nGaps > 24 horas: {len(gaps_grandes)}")
```

### Ejemplo 4: An√°lisis R√°pido de un A√±o

```python
import pandas as pd

# Cargar solo 2024
df_2024 = pd.read_csv('../Procesados/2024/NQ_1min_2024.csv',
                      parse_dates=['DateTime'])

# Estad√≠sticas b√°sicas
print("Estad√≠sticas 2024:")
print(df_2024[['Open', 'High', 'Low', 'Close', 'Volume']].describe())

# Precio promedio por mes
df_2024['Mes'] = df_2024['DateTime'].dt.month
promedio_mensual = df_2024.groupby('Mes')['Close'].mean()
print("\nPrecio promedio por mes:")
print(promedio_mensual)
```

---

## Ap√©ndices

### A. Configuraci√≥n de Rutas

```python
# Archivo: consolidar_datos_minutos.py
DATOS_MINUTOS_PATH = Path("..") / "datos brutos" / "datos ninjatrader" / "Minutos"
OUTPUT_PATH = Path("..") / "Originales"
PROCESSED_PATH = Path("..") / "Procesados"

# Para modificar rutas, editar estas constantes
```

### B. Formato de Logs

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('../Logs/consolidacion_minutos.log'),
        logging.StreamHandler()
    ]
)
```

**Niveles de Log:**
- `INFO`: Informaci√≥n general del proceso
- `WARNING`: Situaciones que requieren atenci√≥n
- `ERROR`: Errores que impiden continuar

### C. Dependencias

```python
# requirements.txt
pandas>=1.3.0
numpy>=1.21.0
```

**Instalaci√≥n:**
```bash
pip install pandas numpy
```

---

**Fin de la Gu√≠a T√©cnica**

*√öltima actualizaci√≥n: 2025-12-04*
*Versi√≥n: 1.0*
